from ultralytics import YOLOE
import cv2
import os
import re
from datetime import datetime
from pytz import timezone
import torch
from torch import nn
from ultralytics.utils.torch_utils import smart_inference_mode
import mobileclip
from ultralytics import MobileCLIP
from time import time_ns
import torch
import supervision as sv
import numpy as np
from tqdm import tqdm 


# –¢–≤–æ–∏ —Ñ—É–Ω–∫—Ü–∏–∏
def get_timestamp(filename):
    match = re.search(r'__CAM_FRONT__(\d{16})\.jpg$', filename)
    return int(match.group(1)) if match else 0


def get_sweeps(jpgspaths, timestamps):
    sweeps = []
    for i in range(len(jpgspaths)):
        if i == 0 or timestamps[i] - timestamps[i-1] > 100000000:  # –ù–æ–≤—ã–π sweep –ø—Ä–∏ —Ä–∞–∑—Ä—ã–≤–µ >100ms
            sweeps.append({'start_idx': i, 'frames': []})
        sweeps[-1]['frames'].append(jpgspaths[i])
    return [s for s in sweeps if len(s['frames']) >= 5]  # –¢–æ–ª—å–∫–æ sweeps —Å >=5 –∫–∞–¥—Ä–∞–º–∏


def frames(sweep):
    return sweep['frames']


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    MAX_SWEEPS = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ sweeps –¥–ª—è –∑–∞–ø–∏—Å–∏ (0 = –≤—Å–µ)
    OUTPUT_DIR = '/mnt/nvme/tmp_output_videos/promt_detection'
    # OUTPUT_DIR = '/mnt/nvme/tmp_output_videos/promt_road'
    FPS = 10  # FPS –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–∏–¥–µ–æ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    prompt = [
        # üöó VEHICLES (–≤—Å–µ —Ç–∏–ø—ã)
        "car", "truck", "bus", "articulated-bus", "school-bus", "tour-bus", 
        "box-truck", "flatbed-truck", "dump-truck", "tanker-truck",
        "delivery-truck", "garbage-truck", "fire-truck", "ambulance", 
        "police-car", "taxi", "van", "minivan", "pickup", "suv",
        
        # üèçÔ∏è TWO-WHEELERS
        "motorcycle", "moped", "scooter", "bicycle", "e-bike", "e-scooter",
        
        # üö∂ PEDESTRIANS (–≤—Å–µ –ø–æ–¥—Ç–∏–ø—ã)
        "pedestrian", "person", "child", "adult", "senior", "police", 
        "firefighter", "construction-worker", "delivery-person",
        "person-sitting", "person-bending", "person-on-phone",
        
        # üêï ANIMALS
        "dog", "cat", "bird", "squirrel", "raccoon", "deer", "coyote", 
        "goat", "pig",
    
        # üö¶ TRAFFIC CONTROL
        "traffic-light", "traffic-light-left", "traffic-light-right", 
        "sign", "dynamic-sign",
        
        # üèóÔ∏è WORK-ZONES
        "construction-cone", "construction-barrel", "construction-barrier", 
        "jersey-barrier", "construction-fence", "construction-sign", 
        "excavator", "backhoe", "crane", "forklift",
        
        # üõë ROAD HAZARDS
        "pothole", "fallen-tree", "debris", "broken-glass", "oil-spill",
        
        # üè™ URBAN
        "fire-hydrant", "parking-meter", "mailbox", "trash-can", "bench", 
        "bike-rack",
        
        # üèõÔ∏è POLES
        "pole", "traffic-pole", "street-light", "light-pole", 
        "sign-pole", "utility-pole", "bollard",
        
        # üè† STRUCTURES
        "bridge", "tunnel", "overpass"
    ]
    # prompt = ["road"]

    device = 'cuda:0'
    model = YOLOE(
        model='/mnt/nvme/huggingface/models--jameslahm--yoloe/snapshots/main/yoloe-11l-seg.pt',
        task='segment',
        verbose=False
    )
    model.to(device)
    model.eval()


    ckptfile='/mnt/nvme/huggingface/models--jameslahm--yoloe/snapshots/main/mobileclip_blt.pt'
    model.set_classes(prompt, model.get_text_pe(prompt, ckptfile))


    jpgsroot = '/mnt/nvme/rowdata/nu/sweeps/CAM_FRONT'
    jpgspaths = [os.path.join(jpgsroot, el) for el in os.listdir(jpgsroot) if el.endswith('.jpg')]
    jpgspaths.sort(key=lambda path: get_timestamp(os.path.basename(path)))
    timestamps = [get_timestamp(os.path.basename(p)) for p in jpgspaths]
    sweeps = get_sweeps(jpgspaths, timestamps)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(sweeps)} sweeps")
    print(f"–ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ sweeps: {min(MAX_SWEEPS, len(sweeps)) if MAX_SWEEPS > 0 else '–≤—Å–µ'}")
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä—ã –æ–¥–∏–Ω —Ä–∞–∑
    mask_annotator = sv.MaskAnnotator(
        color_lookup=sv.ColorLookup.INDEX,
        opacity=0.4
    )
    box_annotator = sv.BoxAnnotator(
        color_lookup=sv.ColorLookup.INDEX,
        thickness=2  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–ª—â–∏–Ω–∞ –¥–ª—è –≤–∏–¥–µ–æ
    )
    label_annotator = sv.LabelAnnotator(
        color_lookup=sv.ColorLookup.INDEX,
        text_scale=0.5,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
        text_thickness=1,
        smart_position=True
    )
    
    total_frames = 0
    total_inference_time = 0
    
    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—É—Ç–µ–π –∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º –≤–∏–¥–µ–æ sweeps
    individual_videos = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º sweeps
    for sweep_idx, sweep in enumerate(sweeps[:MAX_SWEEPS]):
        print(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ Sweep {sweep_idx+1}/{min(MAX_SWEEPS, len(sweeps))}: {len(sweep['frames'])} –∫–∞–¥—Ä–æ–≤ ===")
        
        frame_indices = [jpgspaths.index(path) for path in sweep['frames']]
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–¥–µ–æ writer –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ sweep
        first_frame = cv2.imread(frames(sweep)[0])
        if first_frame is None:
            continue
            
        h, w = first_frame.shape[:2]
        video_path = os.path.join(OUTPUT_DIR, f'sweep_{sweep_idx+1:03d}_{len(sweep["frames"])}frames.mp4')
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(video_path, fourcc, FPS, (w, h))
        
        print(f"–ó–∞–ø–∏—Å—å –≤: {video_path}")
        
        frame_count = 0
        for frame_idx, path in enumerate(frames(sweep)):
            image = cv2.imread(path)
            if image is None:
                continue
                
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            torch.cuda.synchronize()
            t1 = time_ns()
            with torch.no_grad():
                results = model.predict(image_rgb, verbose=False)
            torch.cuda.synchronize()
            t2 = time_ns()
            inference_time = (t2 - t1) / 1e6
            total_inference_time += inference_time
            total_frames += 1
            
            class_names = model.names
            detections = sv.Detections.from_ultralytics(results[0])
            
            labels = [
                f"{class_names[int(cl_id)]}"
                for cl_id, conf in zip(detections.class_id, detections.confidence)
            ]
            
            # –ê–Ω–Ω–æ—Ç–∏—Ä—É–µ–º –∫–∞–¥—Ä
            annotated_image = image_rgb.copy()
            annotated_image = mask_annotator.annotate(scene=annotated_image, detections=detections)
            annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)
            annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)
            
            # BGR –¥–ª—è –∑–∞–ø–∏—Å–∏
            annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–¥—Ä
            out.write(annotated_image_bgr)
            frame_count += 1
            
            if frame_count % 100 == 0:
                print(f"  –ó–∞–ø–∏—Å–∞–Ω–æ {frame_count}/{len(sweep['frames'])} –∫–∞–¥—Ä–æ–≤")
        
        out.release()
        print(f"  Sweep {sweep_idx+1} –∑–∞–≤–µ—Ä—à–µ–Ω: {frame_count} –∫–∞–¥—Ä–æ–≤ –∑–∞–ø–∏—Å–∞–Ω–æ")
        individual_videos.append(video_path)  # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ
    
    # === –°–û–ó–î–ê–ù–ò–ï –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ì–û –í–ò–î–ï–û ===
    print(f"\n=== –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ {len(individual_videos)} —Ñ–∞–π–ª–æ–≤ ===")
    
    # –ë–µ—Ä–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–µ—Ä–≤–æ–≥–æ –≤–∏–¥–µ–æ
    cap = cv2.VideoCapture(individual_videos[0])
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps_combined = FPS  # –¢–æ—Ç –∂–µ FPS
    cap.release()
    
    combined_video_path = os.path.join(OUTPUT_DIR, f'combined_all_sweeps_{len(individual_videos)}_sweeps.mp4')
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    combined_out = cv2.VideoWriter(combined_video_path, fourcc, fps_combined, (w, h))
    
    total_combined_frames = 0
    for video_idx, video_path in enumerate(individual_videos):
        print(f"  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ {video_idx+1}/{len(individual_videos)}: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            combined_out.write(frame)
            frame_count += 1
            total_combined_frames += 1
            
            if frame_count % 100 == 0:
                print(f"    –î–æ–±–∞–≤–ª–µ–Ω–æ {frame_count} –∫–∞–¥—Ä–æ–≤ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ")
        
        cap.release()
        print(f"    –í–∏–¥–µ–æ {video_idx+1} –¥–æ–±–∞–≤–ª–µ–Ω–æ: {frame_count} –∫–∞–¥—Ä–æ–≤")
    
    combined_out.release()
    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {combined_video_path}")
    print(f"–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤ –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º –≤–∏–¥–µ–æ: {total_combined_frames}")
    
    print(f"\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ sweeps: {min(MAX_SWEEPS, len(sweeps))}")
    print(f"–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {total_frames}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {total_inference_time/total_frames:.2f} ms/–∫–∞–¥—Ä")
    print(f"–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}")
    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ: {combined_video_path}")
